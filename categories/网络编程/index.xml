<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>网络编程 on 徐锦平的博客</title>
    <link>http://www.fpstop.com/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/</link>
    <description>Recent content in 网络编程 on 徐锦平的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 21 May 2017 21:44:51 +0800</lastBuildDate>
    <atom:link href="http://www.fpstop.com/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>游戏服务器漫谈 压力测试带来的思考</title>
      <link>http://www.fpstop.com/net/stress_net/</link>
      <pubDate>Sun, 21 May 2017 21:44:51 +0800</pubDate>
      
      <guid>http://www.fpstop.com/net/stress_net/</guid>
      <description>

&lt;p&gt;游戏终于上线啦，加班结束，来总结一下前一段时间的所学吧。先从上线前的压力测试说起。&lt;/p&gt;

&lt;h1 id=&#34;1-网络相关优化&#34;&gt;1 网络相关优化&lt;/h1&gt;

&lt;h2 id=&#34;1-1-背景&#34;&gt;1.1 背景&lt;/h2&gt;

&lt;p&gt;单位的网络库使用的时候，网络和逻辑处理是两个线程。&lt;/p&gt;

&lt;p&gt;网络线程负责：接收数据, 解析出protobuf结构，放入网络队列。&lt;/p&gt;

&lt;p&gt;逻辑线程负责：从网络队列取出protobuf，处理。&lt;/p&gt;

&lt;h2 id=&#34;1-2-思考-网络线程是否有意义&#34;&gt;1.2 思考：网络线程是否有意义？&lt;/h2&gt;

&lt;p&gt;如果网络线程处理完直接处理，其实也是可以的。但是分离网络线程会避免当逻辑线程处理过慢，导致socket接收缓冲区满了，客户端发送数据失败。不过逻辑线程处理太慢也会导致客户端发出请求没有回应。&lt;/p&gt;

&lt;h2 id=&#34;1-3-优化-逻辑线程-每次处理网络队列的数量应该有个上限&#34;&gt;1.3 优化：逻辑线程，每次处理网络队列的数量应该有个上限&lt;/h2&gt;

&lt;p&gt;逻辑线程的主循环伪代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;while(1)
{
    // 处理网络队列里的请求
    while(!pack.empty())
    {
        Msg *p = pack.pop();
        process(p);
    }
    // 一些其他的循环，例如发送ping包，每隔5分钟发送体力，libevent库的循环。。。
    check_ping();
    check_present_energy();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;压力测试时，因为逻辑线程处理的速度比网络线程慢，所以逻辑线程会一直卡在处理网路请求的循环中，因为pack永远不为空，修改方案是在while循环的条件加上一个每次处理包的个数上限，这样其他循环也会有机会执行。&lt;/p&gt;

&lt;p&gt;发现这个问题是因为测试时，服务器的状态数据是用libevent库写的一个http接口获取的，但是测试时发现调用这个接口一直超时，后来发现时因为libevent的循环一直没有执行。&lt;/p&gt;

&lt;h1 id=&#34;2-mysql数据库相关优化&#34;&gt;2 mysql数据库相关优化&lt;/h1&gt;

&lt;p&gt;每个玩家在数据库的角色表是一行数据，每次玩家下线时的保存数据的sql语句是这个样子的：&lt;/p&gt;

&lt;p&gt;updata role_table set val1=xxx,val2=xxx,val3=xxx &amp;hellip;.. where roleid=yyy;&lt;/p&gt;

&lt;p&gt;会更新所有字段无论是否有修改，有时玩家仅仅修改了几个字段却需要更新几十个字段。&lt;/p&gt;

&lt;p&gt;然后做了优化，把玩家的信息在上线时保存了一个备份，下线保存时会比较只把修改的字段拼到update语句中，因为使用的protobuf支持反射，所以可以写一个通用接口，不必担心字段变化。&lt;/p&gt;

&lt;h1 id=&#34;3-redis相关优化&#34;&gt;3 redis相关优化&lt;/h1&gt;

&lt;p&gt;游戏中使用了redis存储了一些数据，因为用的时redis的同步api，每次使用相当于向redis server发送一个请求，然后“原地”等候redis的返回，这里开销还是很大的。&lt;/p&gt;

&lt;p&gt;优化时时采取了redis支持的lua脚本，将一些简单逻辑让redis server来执行，来减少和redis server的网路交互次数。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>socket的属性设置 （持续更新）</title>
      <link>http://www.fpstop.com/redis/learn_src1/</link>
      <pubDate>Fri, 24 Mar 2017 21:47:04 +0800</pubDate>
      
      <guid>http://www.fpstop.com/redis/learn_src1/</guid>
      <description>

&lt;p&gt;今天看redis代码时，发现了如下代码，设置阻塞socket的读写超时时间，仔细一看就是简单的设置了一下socket的属性，索性把socket一些属性总结一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/* Set read/write timeout on a blocking socket. */
int redisSetTimeout(redisContext *c, const struct timeval tv)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;1-读超时&#34;&gt;1 读超时&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;The timeout, in milliseconds, for blocking receive calls. The default for this option is zero, which indicates that a receive operation will not time out. If a blocking receive call times out, the connection is in an indeterminate state and should be closed.
    注意，linux和windows的参数略不同。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;    bool SetRecvTimeOut(uint32 millisecond)
    {
#ifdef WIN32
        DWORD time = millisecond;
        if (setsockopt(sock, SOL_SOCKET, SO_RCVTIMEO, (const char*)&amp;amp;time, sizeof(DWORD)) == -1) {
            return false;
        }
#else
        struct timeval tv;
        tv.tv_sec = millisecond / 1000;
        tv.tv_usec = (millisecond % 1000) * 1000;
        if (setsockopt(sock, SOL_SOCKET, SO_RCVTIMEO, (const char*)&amp;amp;tv, sizeof(tv)) == -1) {
            return false;
        }
#endif
        return true;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-写超时&#34;&gt;2 写超时&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;The timeout, in milliseconds, for blocking send calls. The default for this option is zero, which indicates that a send operation will not time out. If a blocking send call times out, the connection is in an indeterminate state and should be closed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;注意，linux和windows的参数略不同。

    bool SetSendTimeOut(uint32 millisecond)
    {
#ifdef WIN32
        DWORD time = millisecond;
        if (setsockopt(sock, SOL_SOCKET, SO_SNDTIMEO, (const char*)&amp;amp;time, sizeof(DWORD)) == -1) {
            return false;
        }
#else
        struct timeval tv;
        tv.tv_sec = millisecond / 1000;
        tv.tv_usec = (millisecond % 1000) * 1000;
        if (setsockopt(sock, SOL_SOCKET, SO_SNDTIMEO, (const char*)&amp;amp;tv, sizeof(tv)) == -1) {
            return false;
        }
#endif
        return true;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-地址重用&#34;&gt;3 地址重用&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Allows a socket to bind to an address and port already in use. The SO_EXCLUSIVEADDRUSE option can prevent this.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一个端口释放后会等待两分钟之后才能再被使用，SO_REUSEADDR是让端口释放后立即就可以被再次使用。&lt;/p&gt;

&lt;p&gt;p2p打洞时也需要设置这个属性。&lt;/p&gt;

&lt;p&gt;公司的网络库貌似都会设置这个属性呢。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool SetReUseAddr(int v)
{
    int ret = setsockopt(sock, SOL_SOCKET, SO_REUSEADDR, (const char*)&amp;amp;v, sizeof(v));
    return 0 == ret;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-keepalive&#34;&gt;4 keepalive&lt;/h2&gt;

&lt;p&gt;设置心跳包，还可以指定心跳包频率，不过建议还是在逻辑层设计心跳协议，来检查连接存活。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int val = 1;
setsockopt(fd, SOL_SOCKET, SO_KEEPALIVE, &amp;amp;val, sizeof(val))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-tcp-nodelay&#34;&gt;5 TCP_NODELAY&lt;/h2&gt;

&lt;p&gt;TCP_NODELAY和TCP_CORK基本上控制了包的“Nagle化”，这里我们主要讲TCP_NODELAY.Nagle化在这里的含义是采用Nagle算法把较小的包组装为更大的帧。JohnNagle是Nagle算法的发明人，后者就是用他的名字来命名的，他在1984年首次用这种方法来尝试解决福特汽车公司的网络拥塞问题（欲了解详情请参看IETF RFC 896）。他解决的问题就是所谓的silly window syndrome，中文称“愚蠢窗口症候群”，具体含义是，因为普遍终端应用程序每产生一次击键操作就会发送一个包，而典型情况下一个包会拥有一个字节的数据载荷以及40个字节长的包头，于是产生4000%的过载，很轻易地就能令网络发生拥塞,。Nagle化后来成了一种标准并且立即在因特网上得以实现。它现在已经成为缺省配置了，但在我们看来，有些场合下把这一选项关掉也是合乎需要的。&lt;/p&gt;

&lt;p&gt;现在让我们假设某个应用程序发出了一个请求，希望发送小块数据，比如sns游戏中的点击确定按钮。我们可以选择立即发送数据或者等待产生更多的数据然后再一次发送两种策略。如果我们马上发送数据，那么交互性的以及客户/服务器型的应用程序将极大地受益。例如，当我们正在发送一个较短的请求并且等候较大的响应时，相关过载与传输的数据总量相比就会比较低，而且，如果请求立即发出那么响应时间也会快一些。以上操作可以通过设置套接字的TCP_NODELAY选项来完成，这样就禁用了Nagle算法，在nginx中设置tcp_nodelay on,注意放在http标签里。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总之这种把小包组成大包的操作应该由逻辑层来做&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bool SetNoDelay()
{
    int yes = 1;

    if (setsockopt(sock, IPPROTO_TCP, TCP_NODELAY, &amp;amp;yes, sizeof(yes)) == -1) {
        return false;
    }
    return true;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;6-阻塞非阻塞&#34;&gt;6 阻塞非阻塞&lt;/h2&gt;

&lt;p&gt;这个属性应该是最重要最常用的了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    //1 :non block
    int SetNonBlock(int value)
    {
#ifndef WIN32
        int oldflags = ::fcntl(sock, F_GETFL, 0);
        /* If reading the flags failed, return error indication now. */
        if (oldflags == -1)
            return -1;

        /* Set just the flag we want to set. */
        if (value != 0)
            oldflags |= O_NONBLOCK;
        else
            oldflags &amp;amp;= ~O_NONBLOCK;
        /* Store modified flag word in the descriptor. */
        return ::fcntl(m_iSock, F_SETFL, oldflags);
#else
        if (::ioctlsocket(sock, FIONBIO, (u_long FAR*)&amp;amp;value) == SOCKET_ERROR)
        {
            return -1;
        }

        return 0;
#endif
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上文中的例子&lt;a href=&#34;https://github.com/xjp342023125/Code/blob/master/common/XSock.hpp&#34;&gt;XSock.hpp&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
